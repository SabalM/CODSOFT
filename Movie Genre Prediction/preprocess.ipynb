{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sabharjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_overview(mydata, overview_col):\n",
    "    # Function to clean show overview\n",
    "    # Return: each row as a list of tokens\n",
    "\n",
    "    # removes punctuation\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    stop_words = stopwords.words(\"English\")\n",
    "\n",
    "    # split text\n",
    "    tokens = mydata[overview_col].map(lambda x: tokenizer.tokenize(str(x)))\n",
    "\n",
    "    # strip white spaces & lower case\n",
    "    tokens = tokens.map(lambda x: [i.lower().strip(\"_\") for i in x])\n",
    "\n",
    "    # remove stop words\n",
    "    tokens = tokens.map(lambda x: [i for i in x if i not in stop_words])\n",
    "\n",
    "    # remove empty strings\n",
    "    tokens = tokens.map(lambda x: [i for i in x if i != ''])\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(train_file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split_line = line.strip().split(' ::: ')\n",
    "                data.append(split_line)\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError: {e}\")\n",
    "\n",
    "    columns = ['ID', 'TITLE', 'GENRE', 'DESCRIPTION']\n",
    "    rows = data[1:]\n",
    "    train_data = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    description_column_name = 'DESCRIPTION'\n",
    "    if description_column_name in train_data.columns:\n",
    "        train_data['TOKENIZED'] = tokenize_overview(train_data, description_column_name)\n",
    "        train_data['CLEANED_DESCRIPTION'] = train_data['TOKENIZED'].map(lambda x: ' '.join(x))\n",
    "    else:\n",
    "        print(f\"The specified column '{description_column_name}' does not exist in the dataset.\")\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split_line = line.strip().split(' ::: ')\n",
    "                data.append(split_line)\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError: {e}\")\n",
    "\n",
    "    columns = ['ID', 'TITLE', 'DESCRIPTION']\n",
    "    rows = data[1:]\n",
    "    test_data = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    description_column_name = 'DESCRIPTION'\n",
    "    if description_column_name in test_data.columns:\n",
    "        test_data['TOKENIZED'] = tokenize_overview(test_data, description_column_name)\n",
    "        test_data['CLEANED_DESCRIPTION'] = test_data['TOKENIZED'].map(lambda x: ' '.join(x))\n",
    "    else:\n",
    "        print(f\"The specified column '{description_column_name}' does not exist in the dataset.\")\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_file_path = 'dataset/train_data.txt'\n",
    "    test_file_path = 'dataset/test_data.txt'\n",
    "\n",
    "    processed_train_data = preprocess_train_data(train_file_path)\n",
    "    processed_test_data = preprocess_test_data(test_file_path)\n",
    "\n",
    "    processed_train_data.to_csv('dataset/processed_train_data.csv', index=False)\n",
    "    processed_test_data.to_csv('dataset/processed_test_data.csv', index=False)\n",
    "\n",
    "    print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Columns: Index(['ID', 'TITLE', 'GENRE', 'DESCRIPTION'], dtype='object')\n",
      "First few rows of data:\n",
      "  ID                             TITLE        GENRE  \\\n",
      "0  2                      Cupid (1997)     thriller   \n",
      "1  3  Young, Wild and Wonderful (1980)        adult   \n",
      "2  4             The Secret Sin (1915)        drama   \n",
      "3  5            The Unrecovered (2007)        drama   \n",
      "4  6            Quality Control (2011)  documentary   \n",
      "\n",
      "                                         DESCRIPTION  \n",
      "0  A brother and sister with a past incestuous re...  \n",
      "1  As the bus empties the students for their fiel...  \n",
      "2  To help their unemployed father make ends meet...  \n",
      "3  The film's title refers not only to the un-rec...  \n",
      "4  Quality Control consists of a series of 16mm s...  \n"
     ]
    }
   ],
   "source": [
    "# Manually set the column names\n",
    "columns = ['ID', 'TITLE', 'GENRE', 'DESCRIPTION']\n",
    "rows = data[1:]  # The rest are data rows\n",
    "mydata = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Print the first few rows and columns to debug\n",
    "print(\"Data Columns:\", mydata.columns)\n",
    "print(\"First few rows of data:\")\n",
    "print(mydata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [brother, sister, past, incestuous, relationsh...\n",
      "1    [bus, empties, students, field, trip, museum, ...\n",
      "2    [help, unemployed, father, make, ends, meet, e...\n",
      "3    [film, title, refers, un, recovered, bodies, g...\n",
      "4    [quality, control, consists, series, 16mm, sin...\n",
      "Name: DESCRIPTION, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_overviews = tokenize_overview(mydata, description_column_name)\n",
    "print(tokenized_overviews.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
